{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "028123e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# 1. Load and Split the Ramayana PDF\n",
    "loader = PyMuPDFLoader(\"data/RAMAYANA.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "744a9db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create a vector store\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\",\n",
    "                                        model_kwargs={\"device\": \"cpu\"})\n",
    "vectorstore = FAISS.from_documents(splits, embedding)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c16783d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define LLM and RetrievalQA chain\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4\")\n",
    "retriever = vectorstore.as_retriever()\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c502b381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Generated Answer ===\n",
      "Rama went to the forest to carry out his father's promises. It was his duty to fulfill these promises, despite the hardship it would cause him.\n"
     ]
    }
   ],
   "source": [
    "# 4. Ask a question\n",
    "query = \"Why did Rama go to the forest?\"\n",
    "response = qa_chain(query)\n",
    "generated_answer = response['result']\n",
    "context_pages = response['source_documents']\n",
    "\n",
    "# Show response\n",
    "print(\"\\n=== Generated Answer ===\")\n",
    "print(generated_answer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d7b5a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Prepare the evaluation prompt\n",
    "scoring_prompt_template = PromptTemplate.from_template(\"\"\"\n",
    "You are a strict evaluator. Your task is to evaluate the quality of the following answer on 3 criteria:\n",
    "\n",
    "1. Completeness (Is the answer complete and does it cover all relevant aspects?)\n",
    "2. Factuality (Is the answer factually correct based on the context provided?)\n",
    "3. Fluency (Is the answer grammatically correct, coherent, and easy to read?)\n",
    "\n",
    "Return your result in JSON format as follows:\n",
    "{{\n",
    "  \"completeness\": \"<score out of 5>\",\n",
    "  \"factuality\": \"<score out of 5>\",\n",
    "  \"fluency\": \"<score out of 5>\",\n",
    "  \"comments\": \"<brief feedback>\"\n",
    "}}\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### Answer:\n",
    "{answer}\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca2c3966",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31057/803609523.py:7: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  scoring_result = scoring_llm.predict(scoring_prompt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LLM-Based Scoring ===\n",
      "{\n",
      "  \"completeness\": \"3 out of 5\",\n",
      "  \"factuality\": \"5 out of 5\",\n",
      "  \"fluency\": \"5 out of 5\",\n",
      "  \"comments\": \"The answer is factually correct and fluently written, but it does not cover all aspects of the context. It does not mention Rama's mother's sadness, Bharatha's rule over Ayodhya, or Rama's decision to leave Chitrkut.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# 6. Prepare context for scoring\n",
    "context_text = \"\\n\\n\".join([doc.page_content[:1000] for doc in context_pages[:2]])  # Limit for brevity\n",
    "scoring_prompt = scoring_prompt_template.format(context=context_text, answer=generated_answer)\n",
    "\n",
    "# 7. Score the generated answer\n",
    "scoring_llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n",
    "scoring_result = scoring_llm.predict(scoring_prompt)\n",
    "\n",
    "# 8. Display scoring results\n",
    "print(\"\\n=== LLM-Based Scoring ===\")\n",
    "print(scoring_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

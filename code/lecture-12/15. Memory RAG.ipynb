{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e189f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "import os\n",
    "\n",
    "\n",
    "# Step 1: Prepare the knowledge base\n",
    "loader = PyMuPDFLoader(\"data/RAMAYANA.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Chunking\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "file = open('data/key.txt', 'r')\n",
    "# Read the API key from the file\n",
    "api_key = file.read().strip()\n",
    "# Close the file\n",
    "file.close()\n",
    "# ðŸ§  Set your OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da22d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings + FAISS Vectorstore\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\",\n",
    "                                        model_kwargs={\"device\": \"cpu\"})\n",
    "vectorstore = FAISS.from_documents(texts, embedding)\n",
    "\n",
    "# Step 2: Setup memory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    "    output_key=\"answer\"  # Specify which output to store in memory\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d1dc2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Initialize LLM and Retrieval Chain with memory\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
    "\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    memory=memory,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6f7527c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1: Rama, also known as Ramachandra, is a major deity in Hinduism. He is the seventh avatar of the god Vishnu and is also considered to be a supreme being in his own right. Rama is the protagonist of the Hindu epic Ramayana, which narrates his idealistic virtues and his adventures. In the provided context, Rama is depicted as a prince who is committed to fulfilling his father's promises, even at the cost of his own comfort and happiness.\n",
      "Response 2: The father of Rama, the major deity in Hinduism, is King Dasharatha.\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Run queries\n",
    "query1 = \"Who is Rama?\"\n",
    "result1 = qa_chain({\"question\": query1})\n",
    "print(\"Response 1:\", result1[\"answer\"])\n",
    "\n",
    "query2 = \"Tell me about his father?\"\n",
    "result2 = qa_chain({\"question\": query2})\n",
    "print(\"Response 2:\", result2[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

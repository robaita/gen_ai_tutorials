{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96a4bf57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/avinash/Desktop/dr_avinash/generative_ai/lecture-8/torch/lib/python3.12/site-packages (3.9.1)\n",
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /home/avinash/Desktop/dr_avinash/generative_ai/lecture-8/torch/lib/python3.12/site-packages (1.6.1)\n",
      "Requirement already satisfied: click in /home/avinash/Desktop/dr_avinash/generative_ai/lecture-8/torch/lib/python3.12/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /home/avinash/Desktop/dr_avinash/generative_ai/lecture-8/torch/lib/python3.12/site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/avinash/Desktop/dr_avinash/generative_ai/lecture-8/torch/lib/python3.12/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /home/avinash/Desktop/dr_avinash/generative_ai/lecture-8/torch/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
      "Collecting absl-py (from rouge-score)\n",
      "  Downloading absl_py-2.3.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: numpy in /home/avinash/Desktop/dr_avinash/generative_ai/lecture-8/torch/lib/python3.12/site-packages (from rouge-score) (2.1.2)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/avinash/Desktop/dr_avinash/generative_ai/lecture-8/torch/lib/python3.12/site-packages (from rouge-score) (1.17.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/avinash/Desktop/dr_avinash/generative_ai/lecture-8/torch/lib/python3.12/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/avinash/Desktop/dr_avinash/generative_ai/lecture-8/torch/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Downloading absl_py-2.3.0-py3-none-any.whl (135 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.7/135.7 kB\u001b[0m \u001b[31m949.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24987 sha256=4958295f11716bb966e802e991d8d13ff94cb8deef95de1cba18c2a80461b193\n",
      "  Stored in directory: /home/avinash/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: absl-py, rouge-score\n",
      "Successfully installed absl-py-2.3.0 rouge-score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install nltk rouge-score scikit-learn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00c742a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/avinash/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f2c7529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Step 1: Load and Chunk Ramayana\n",
    "loader = PyMuPDFLoader(\"data/RAMAYANA.pdf\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72e112a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "# Step 2: Create FAISS VectorStore\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\",\n",
    "                                        model_kwargs={\"device\": \"cpu\"})\n",
    "\n",
    "vectorstore = FAISS.from_documents(chunks, embedding)\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f3544f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_data = [\n",
    "  {\n",
    "    \"question\": \"Who were the parents of Lord Rama?\",\n",
    "    \"answer\": \"Lord Rama was born to King Dasharatha and Queen Kausalya.\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"What was Kaikeyi's wish to King Dasharatha?\",\n",
    "    \"answer\": \"Kaikeyi asked Dasharatha to exile Rama and make Bharata the king.\"\n",
    "  }\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0fe39843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Build QA Chain\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76739ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7210c797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Evaluation Report:\n",
      "\n",
      "1. ❓ Who were the parents of Lord Rama?\n",
      "🔹 Ground Truth: Lord Rama was born to King Dasharatha and Queen Kausalya.\n",
      "🔸 Prediction  : The parents of Lord Rama were King Dasharatha and Queen Kaushalya.\n",
      "📐 Cosine Sim  : 0.4039\n",
      "🟦 BLEU Score  : 0.2620\n",
      "🟥 ROUGE-L     : 0.5714\n",
      "📊 Precision   : 0.5833, Recall: 0.6364, F1: 0.6087\n",
      "\n",
      "2. ❓ What was Kaikeyi's wish to King Dasharatha?\n",
      "🔹 Ground Truth: Kaikeyi asked Dasharatha to exile Rama and make Bharata the king.\n",
      "🔸 Prediction  : Kaikeyi's wish to King Dasharatha was to have her son, Bharata, crowned as the king instead of Rama, and for Rama to be exiled to the forest for fourteen years.\n",
      "📐 Cosine Sim  : 0.4564\n",
      "🟦 BLEU Score  : 0.0164\n",
      "🟥 ROUGE-L     : 0.2857\n",
      "📊 Precision   : 0.3462, Recall: 0.7500, F1: 0.4737\n",
      "\n",
      "📈 Summary Metrics:\n",
      "🔁 Total Samples    : 2\n",
      "📐 Avg Cosine Sim   : 0.4302\n",
      "🟦 Avg BLEU Score   : 0.1392\n",
      "🟥 Avg ROUGE-L      : 0.4286\n",
      "📊 Avg Precision    : 0.4647\n",
      "📊 Avg Recall       : 0.6932\n",
      "📊 Avg F1-Score     : 0.5412\n",
      "✅ Accuracy (Sim>0.9) : 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Scoring Functions\n",
    "def compute_cosine_similarity(pred, truth):\n",
    "    vect = TfidfVectorizer().fit([pred, truth])\n",
    "    vecs = vect.transform([pred, truth])\n",
    "    return cosine_similarity(vecs[0], vecs[1])[0][0]\n",
    "\n",
    "rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "bleu_smooth = SmoothingFunction().method1\n",
    "\n",
    "# Step 6: Evaluation Loop\n",
    "similarities, precisions, recalls, f1s, bleus, rouges = [], [], [], [], [], []\n",
    "binary_labels, binary_preds = [], []\n",
    "\n",
    "THRESHOLD = 0.9  # Define similarity threshold for classification\n",
    "\n",
    "print(\"\\n📊 Evaluation Report:\")\n",
    "for i, item in enumerate(ground_truth_data, 1):\n",
    "    q, truth = item[\"question\"], item[\"answer\"]\n",
    "    pred = qa_chain.run(q)\n",
    "\n",
    "    # Tokenize\n",
    "    ref_tokens = nltk.word_tokenize(truth.lower())\n",
    "    pred_tokens = nltk.word_tokenize(pred.lower())\n",
    "\n",
    "    # Metrics\n",
    "    sim = compute_cosine_similarity(pred, truth)\n",
    "    bleu = sentence_bleu([ref_tokens], pred_tokens, smoothing_function=bleu_smooth)\n",
    "    rouge_l = rouge.score(pred, truth)['rougeL'].fmeasure\n",
    "\n",
    "    precision = len(set(pred_tokens) & set(ref_tokens)) / (len(set(pred_tokens)) + 1e-5)\n",
    "    recall = len(set(pred_tokens) & set(ref_tokens)) / (len(set(ref_tokens)) + 1e-5)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-5)\n",
    "\n",
    "    # Record metrics\n",
    "    similarities.append(sim)\n",
    "    bleus.append(bleu)\n",
    "    rouges.append(rouge_l)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1s.append(f1)\n",
    "    binary_labels.append(1)  # All ground truth are positive\n",
    "    binary_preds.append(1 if sim >= THRESHOLD else 0)\n",
    "\n",
    "    print(f\"\\n{i}. ❓ {q}\")\n",
    "    print(f\"🔹 Ground Truth: {truth}\")\n",
    "    print(f\"🔸 Prediction  : {pred}\")\n",
    "    print(f\"📐 Cosine Sim  : {sim:.4f}\")\n",
    "    print(f\"🟦 BLEU Score  : {bleu:.4f}\")\n",
    "    print(f\"🟥 ROUGE-L     : {rouge_l:.4f}\")\n",
    "    print(f\"📊 Precision   : {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "# Final Aggregates\n",
    "print(\"\\n📈 Summary Metrics:\")\n",
    "print(f\"🔁 Total Samples    : {len(ground_truth_data)}\")\n",
    "print(f\"📐 Avg Cosine Sim   : {np.mean(similarities):.4f}\")\n",
    "print(f\"🟦 Avg BLEU Score   : {np.mean(bleus):.4f}\")\n",
    "print(f\"🟥 Avg ROUGE-L      : {np.mean(rouges):.4f}\")\n",
    "print(f\"📊 Avg Precision    : {np.mean(precisions):.4f}\")\n",
    "print(f\"📊 Avg Recall       : {np.mean(recalls):.4f}\")\n",
    "print(f\"📊 Avg F1-Score     : {np.mean(f1s):.4f}\")\n",
    "print(f\"✅ Accuracy (Sim>{THRESHOLD}) : {accuracy_score(binary_labels, binary_preds):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3e5a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Evaluation Metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def compute_similarity(a, b):\n",
    "    vect = TfidfVectorizer().fit([a, b])\n",
    "    vecs = vect.transform([a, b])\n",
    "    score = cosine_similarity(vecs[0], vecs[1])[0][0]\n",
    "    return round(score, 4)\n",
    "\n",
    "def compute_similarity_semantic(a, b):\n",
    "    vect = TfidfVectorizer().fit([a, b])\n",
    "    vecs = vect.transform([a, b])\n",
    "    score = cosine_similarity(vecs[0], vecs[1])[0][0]\n",
    "    return round(score, 4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09129652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Evaluation Report:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30737/2911884050.py:10: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  prediction = qa_chain.run(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. ❓ Question: Who were the parents of Lord Rama?\n",
      "🔹 Ground Truth: Lord Rama was born to King Dasharatha and Queen Kausalya.\n",
      "🔸 Prediction: The parents of Lord Rama were King Dasharatha and Queen Kaushalya.\n",
      "✅ Similarity Score: 0.4039\n",
      "\n",
      "2. ❓ Question: What was Kaikeyi's wish to King Dasharatha?\n",
      "🔹 Ground Truth: Kaikeyi asked Dasharatha to exile Rama and make Bharata the king.\n",
      "🔸 Prediction: Kaikeyi's wish to King Dasharatha was to have her son, Bharata, crowned as the king instead of Rama, and for Rama to be banished to live in the forest like a hermit for fourteen years.\n",
      "✅ Similarity Score: 0.4319\n"
     ]
    }
   ],
   "source": [
    "total = len(ground_truth_data)\n",
    "exact_match = 0\n",
    "similarity_scores = []\n",
    "\n",
    "print(\"\\n📊 Evaluation Report:\")\n",
    "for i, item in enumerate(ground_truth_data, 1):\n",
    "    question = item[\"question\"]\n",
    "    ground_truth = item[\"answer\"]\n",
    "\n",
    "    prediction = qa_chain.run(question)\n",
    "\n",
    "    similarity = compute_similarity(prediction, ground_truth)\n",
    "    similarity_scores.append(similarity)\n",
    "\n",
    "    if similarity > 0.95:\n",
    "        exact_match += 1\n",
    "\n",
    "    print(f\"\\n{i}. ❓ Question: {question}\")\n",
    "    print(f\"🔹 Ground Truth: {ground_truth}\")\n",
    "    print(f\"🔸 Prediction: {prediction}\")\n",
    "    print(f\"✅ Similarity Score: {similarity}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "768c93cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Summary:\n",
      "🔁 Total Questions: 2\n",
      "🎯 Exact Matches (Similarity > 0.95): 0\n",
      "📘 Average Similarity Score: 0.4179\n",
      "⚠️ Hallucination Rate: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Final Metrics\n",
    "avg_similarity = sum(similarity_scores) / total\n",
    "hallucination_rate = round(1 - (exact_match / total), 2)\n",
    "\n",
    "print(\"\\n📈 Summary:\")\n",
    "print(f\"🔁 Total Questions: {total}\")\n",
    "print(f\"🎯 Exact Matches (Similarity > 0.95): {exact_match}\")\n",
    "print(f\"📘 Average Similarity Score: {round(avg_similarity, 4)}\")\n",
    "print(f\"⚠️ Hallucination Rate: {hallucination_rate}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b82299fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chat_models import ChatOpenAI\n",
    "# from langchain.prompts import PromptTemplate\n",
    "# from langchain.embeddings import HuggingFaceEmbeddings\n",
    "# from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d222d406",
   "metadata": {},
   "source": [
    "# Dynamic Header Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ca5836b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk = \"ChatGPT is an advanced AI language model developed by OpenAI, based on the GPT (Generative Pre-trained Transformer) architecture. It is designed to understand and generate human-like text based on the input it receives. ChatGPT can engage in conversations, answer questions, write content, summarize information, and assist with a variety of tasks across domains. Trained on large datasets from the internet, it leverages deep learning to provide contextually relevant responses. Ideal for both casual use and professional applications, ChatGPT represents a significant step forward in natural language processing, making human-computer interaction more intuitive and accessible than ever before.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca484cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26445/584157652.py:15: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  return llm.predict(header_prompt.format(content=text))\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "header_prompt = PromptTemplate(\n",
    "    input_variables=[\"content\"],\n",
    "    template=\"Create a short, descriptive title for the following content:\\n\\n{content}\"\n",
    ")\n",
    "\n",
    "def generate_header(text):\n",
    "    return llm.predict(header_prompt.format(content=text))\n",
    "\n",
    "header = generate_header(chunk)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcf0ccc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header: \"ChatGPT: OpenAI's Advanced AI Language Model Revolutionizing Human-Computer Interaction\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Header:\", header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98790242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine header with chunk\n",
    "chunk_with_header = f\"Header: {header}\\n\\nContent: {chunk}\"\n",
    "# Create LangChain document\n",
    "doc = Document(page_content=chunk_with_header, metadata={\"header\": header})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b631df",
   "metadata": {},
   "source": [
    "# Embedding-Aware Section Titles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "888e4025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggested Title: Transformer Models\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\", device='cpu')\n",
    "\n",
    "topics = [\"Transformer Models\", \"Language Generation\", \"Chatbot Applications\"]\n",
    "topic_embeddings = model.encode(topics, convert_to_tensor=True)\n",
    "\n",
    "chunk_text = \"Transformers use attention mechanisms to weigh importance of tokens...\"\n",
    "chunk_embedding = model.encode(chunk_text, convert_to_tensor=True)\n",
    "\n",
    "# Find most similar topic\n",
    "cos_scores = util.pytorch_cos_sim(chunk_embedding, topic_embeddings)\n",
    "best_topic = topics[cos_scores.argmax()]\n",
    "print(\"Suggested Title:\", best_topic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fdaf10",
   "metadata": {},
   "source": [
    "# Summarization-Based Token Reduction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0557524a",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_chunk_text = \"\"\"Transformers have revolutionized the field of machine learning, particularly in Natural Language Processing (NLP). Introduced in the 2017 paper \"Attention Is All You Need,\" they fundamentally changed how models process sequential data like text.\n",
    "\n",
    "Unlike earlier recurrent neural networks (RNNs) and long short-term memory (LSTMs) that processed words sequentially, Transformers leverage a mechanism called self-attention. This allows them to weigh the importance of different words in a sequence simultaneously, regardless of their position. For example, in the sentence \"The quick brown fox jumped over the lazy dog,\" a Transformer can understand the relationship between \"fox\" and \"dog\" directly, without having to process all the words in between step-by-step.\n",
    "\n",
    "The core components of a Transformer include encoders and decoders. Encoders process the input sequence, while decoders generate the output. Both consist of multiple layers, each containing multi-head self-attention and position-wise feed-forward networks. Positional encodings are added to the input embeddings to provide information about the order of words, as the parallel processing inherent in self-attention would otherwise lose this crucial context.\n",
    "\n",
    "This parallel processing capability is a major advantage, leading to significantly faster training times and enabling the development of massive models like BERT and GPT. Transformers have since expanded beyond NLP into computer vision (Vision Transformers or ViTs), revolutionizing tasks like image classification and object detection. Their ability to capture long-range dependencies and handle large datasets has made them the backbone of modern AI, driving advancements in areas like text generation, machine translation, and even protein folding prediction.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "732b51c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Chunk Size: 1786\n",
      "Summarized Chunk Size: 586\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "print(\"Large Chunk Size:\", len(large_chunk_text))\n",
    "\n",
    "docs = [Document(page_content=large_chunk_text)]\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "summary = chain.run(docs)\n",
    "# print(\"Summarized Chunk:\", summary)\n",
    "\n",
    "print(\"Summarized Chunk Size:\", len(summary))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48290c25",
   "metadata": {},
   "source": [
    "# Importance-Aware Trimming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e398ac26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimmed Chunk: GPT was released in 2018. GPT is transformative for NLP. It can summarize text.\n"
     ]
    }
   ],
   "source": [
    "long_chunk_text = \"GPT was released in 2018. OpenAI is its developer. \" \\\n",
    "\"GPT is transformative for NLP. It can summarize text.\"\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def importance_trim(text, top_n=3):\n",
    "    sentences = text.split('. ')\n",
    "    tfidf = TfidfVectorizer().fit_transform(sentences)\n",
    "    scores = tfidf.sum(axis=1).A1\n",
    "    top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_n]\n",
    "    return '. '.join([sentences[i] for i in top_indices])\n",
    "\n",
    "trimmed = importance_trim(long_chunk_text)\n",
    "print(\"Trimmed Chunk:\", trimmed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610a8515",
   "metadata": {},
   "source": [
    "# LLM-Assisted Compression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f42f733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed Output: Developed by OpenAI, GPT-4 stands as one of the most potent models, benefiting from extensive training on expansive data sets.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "chunk_text = \"GPT-4, developed by OpenAI, is one of the most powerful models, trained on vast data...\"\n",
    "\n",
    "compression_prompt = PromptTemplate(\n",
    "    input_variables=[\"content\"],\n",
    "    template=\"Compress the following content into a concise, information-dense paragraph:\\n\\n{content}\"\n",
    ")\n",
    "\n",
    "def compress_text(content):\n",
    "    return llm.predict(compression_prompt.format(content=content))\n",
    "\n",
    "compressed = compress_text(chunk_text)\n",
    "print(\"Compressed Output:\", compressed)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

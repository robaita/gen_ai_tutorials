{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPm93MRJm7WjzstjYV1LpGR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6LVDT2JCp43K","executionInfo":{"status":"ok","timestamp":1748769459554,"user_tz":-330,"elapsed":7068,"user":{"displayName":"Dr. Avinash Kumar Singh","userId":"03688237427667000371"}},"outputId":"366ec717-54fa-48a6-b686-cb2eb07dbba4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torchviz in /usr/local/lib/python3.11/dist-packages (0.0.3)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from torchviz) (2.6.0+cu124)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from torchviz) (0.20.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->torchviz) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->torchviz) (3.0.2)\n"]}],"source":["# !huggingface-cli login\n","# !pip install torchview\n","# !pip install torchinfo\n","!pip install torchviz\n"]},{"cell_type":"code","source":["import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","model_id = \"meta-llama/Llama-3.2-1B\"\n","\n","# Load the model\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    torch_dtype=torch.bfloat16,\n","    device_map=\"auto\"\n",")\n","\n","# Print the model architecture\n","print(model)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RpKzxdVmt3Bg","executionInfo":{"status":"ok","timestamp":1748769506625,"user_tz":-330,"elapsed":26630,"user":{"displayName":"Dr. Avinash Kumar Singh","userId":"03688237427667000371"}},"outputId":"955de005-dd35-426e-e43b-012f93d5c41e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["LlamaForCausalLM(\n","  (model): LlamaModel(\n","    (embed_tokens): Embedding(128256, 2048)\n","    (layers): ModuleList(\n","      (0-15): 16 x LlamaDecoderLayer(\n","        (self_attn): LlamaAttention(\n","          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n","          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n","          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n","          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n","        )\n","        (mlp): LlamaMLP(\n","          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n","          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n","          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n","          (act_fn): SiLU()\n","        )\n","        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n","        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n","      )\n","    )\n","    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n","    (rotary_emb): LlamaRotaryEmbedding()\n","  )\n","  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",")\n"]}]},{"cell_type":"code","source":["from torchviz import make_dot\n","# --- Add the visualization part ---\n","# Create a dummy input tensor matching the expected input shape for a causal language model\n","# A typical input for a text model is a sequence of token IDs.\n","# Let's assume a batch size of 1 and a sequence length (e.g., 10 tokens)\n","dummy_input = torch.randint(0, model.config.vocab_size, (1, 10)).to(model.device)\n","\n","# Generate the graph\n","dot = make_dot(model(dummy_input).logits, params=dict(model.named_parameters()))\n","\n","# Save the diagram to a file (e.g., PDF or PNG)\n","dot.render(\"llama_architecture\", format=\"pdf\", view=True) # or format=\"png\"\n","\n","print(\"Model architecture diagram saved as 'llama_architecture.pdf'\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aYeI_Ya1uz18","executionInfo":{"status":"ok","timestamp":1748769518945,"user_tz":-330,"elapsed":5602,"user":{"displayName":"Dr. Avinash Kumar Singh","userId":"03688237427667000371"}},"outputId":"87c27945-9260-4041-d858-f7ebc30bfc1a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model architecture diagram saved as 'llama_architecture.pdf'\n"]}]},{"cell_type":"code","source":["from transformers import AutoConfig\n","\n","config = AutoConfig.from_pretrained(model_id)\n","print(config)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gq_7V2j8p_W5","executionInfo":{"status":"ok","timestamp":1748768468395,"user_tz":-330,"elapsed":103,"user":{"displayName":"Dr. Avinash Kumar Singh","userId":"03688237427667000371"}},"outputId":"023c6d62-e931-41af-9149-0d090be47a36"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["LlamaConfig {\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": 128001,\n","  \"head_dim\": 64,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 2048,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 8192,\n","  \"max_position_embeddings\": 131072,\n","  \"mlp_bias\": false,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 16,\n","  \"num_key_value_heads\": 8,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": {\n","    \"factor\": 32.0,\n","    \"high_freq_factor\": 4.0,\n","    \"low_freq_factor\": 1.0,\n","    \"original_max_position_embeddings\": 8192,\n","    \"rope_type\": \"llama3\"\n","  },\n","  \"rope_theta\": 500000.0,\n","  \"tie_word_embeddings\": true,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.52.2\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128256\n","}\n","\n"]}]},{"cell_type":"code","source":["for name, module in model.named_modules():\n","    print(name, \"->\", type(module))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uLmF-NLnqBHx","executionInfo":{"status":"ok","timestamp":1748768483401,"user_tz":-330,"elapsed":61,"user":{"displayName":"Dr. Avinash Kumar Singh","userId":"03688237427667000371"}},"outputId":"db317873-6984-45b5-d79e-a2be2a8e3b25"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" -> <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n","model -> <class 'transformers.models.llama.modeling_llama.LlamaModel'>\n","model.embed_tokens -> <class 'torch.nn.modules.sparse.Embedding'>\n","model.layers -> <class 'torch.nn.modules.container.ModuleList'>\n","model.layers.0 -> <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n","model.layers.0.self_attn -> <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n","model.layers.0.self_attn.q_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.0.self_attn.k_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.0.self_attn.v_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.0.self_attn.o_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.0.mlp -> <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n","model.layers.0.mlp.gate_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.0.mlp.up_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.0.mlp.down_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.0.mlp.act_fn -> <class 'torch.nn.modules.activation.SiLU'>\n","model.layers.0.input_layernorm -> <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n","model.layers.0.post_attention_layernorm -> <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n","model.layers.1 -> <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n","model.layers.1.self_attn -> <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n","model.layers.1.self_attn.q_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.1.self_attn.k_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.1.self_attn.v_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.1.self_attn.o_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.1.mlp -> <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n","model.layers.1.mlp.gate_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.1.mlp.up_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.1.mlp.down_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.1.mlp.act_fn -> <class 'torch.nn.modules.activation.SiLU'>\n","model.layers.1.input_layernorm -> <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n","model.layers.1.post_attention_layernorm -> <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n","model.layers.2 -> <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n","model.layers.2.self_attn -> <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n","model.layers.2.self_attn.q_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.2.self_attn.k_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.2.self_attn.v_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.2.self_attn.o_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.2.mlp -> <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n","model.layers.2.mlp.gate_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.2.mlp.up_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.2.mlp.down_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.2.mlp.act_fn -> <class 'torch.nn.modules.activation.SiLU'>\n","model.layers.2.input_layernorm -> <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n","model.layers.2.post_attention_layernorm -> <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n","model.layers.3 -> <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n","model.layers.3.self_attn -> <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n","model.layers.3.self_attn.q_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.3.self_attn.k_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.3.self_attn.v_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.3.self_attn.o_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.3.mlp -> <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n","model.layers.3.mlp.gate_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.3.mlp.up_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.3.mlp.down_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.3.mlp.act_fn -> <class 'torch.nn.modules.activation.SiLU'>\n","model.layers.3.input_layernorm -> <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n","model.layers.3.post_attention_layernorm -> <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n","model.layers.4 -> <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n","model.layers.4.self_attn -> <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n","model.layers.4.self_attn.q_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.4.self_attn.k_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.4.self_attn.v_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.4.self_attn.o_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.4.mlp -> <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n","model.layers.4.mlp.gate_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.4.mlp.up_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.4.mlp.down_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.4.mlp.act_fn -> <class 'torch.nn.modules.activation.SiLU'>\n","model.layers.4.input_layernorm -> <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n","model.layers.4.post_attention_layernorm -> <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n","model.layers.5 -> <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n","model.layers.5.self_attn -> <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n","model.layers.5.self_attn.q_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.5.self_attn.k_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.5.self_attn.v_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.5.self_attn.o_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.5.mlp -> <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n","model.layers.5.mlp.gate_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.5.mlp.up_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.5.mlp.down_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.5.mlp.act_fn -> <class 'torch.nn.modules.activation.SiLU'>\n","model.layers.5.input_layernorm -> <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n","model.layers.5.post_attention_layernorm -> <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n","model.layers.6 -> <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n","model.layers.6.self_attn -> <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n","model.layers.6.self_attn.q_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.6.self_attn.k_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.6.self_attn.v_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.6.self_attn.o_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.6.mlp -> <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n","model.layers.6.mlp.gate_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.6.mlp.up_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.6.mlp.down_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.6.mlp.act_fn -> <class 'torch.nn.modules.activation.SiLU'>\n","model.layers.6.input_layernorm -> <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n","model.layers.6.post_attention_layernorm -> <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n","model.layers.7 -> <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n","model.layers.7.self_attn -> <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n","model.layers.7.self_attn.q_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.7.self_attn.k_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.7.self_attn.v_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.7.self_attn.o_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.7.mlp -> <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n","model.layers.7.mlp.gate_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.7.mlp.up_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.7.mlp.down_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.7.mlp.act_fn -> <class 'torch.nn.modules.activation.SiLU'>\n","model.layers.7.input_layernorm -> <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n","model.layers.7.post_attention_layernorm -> <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n","model.layers.8 -> <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n","model.layers.8.self_attn -> <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n","model.layers.8.self_attn.q_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.8.self_attn.k_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.8.self_attn.v_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.8.self_attn.o_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.8.mlp -> <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n","model.layers.8.mlp.gate_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.8.mlp.up_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.8.mlp.down_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.8.mlp.act_fn -> <class 'torch.nn.modules.activation.SiLU'>\n","model.layers.8.input_layernorm -> <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n","model.layers.8.post_attention_layernorm -> <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n","model.layers.9 -> <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n","model.layers.9.self_attn -> <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n","model.layers.9.self_attn.q_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.9.self_attn.k_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.9.self_attn.v_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.9.self_attn.o_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.9.mlp -> <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n","model.layers.9.mlp.gate_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.9.mlp.up_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.9.mlp.down_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.9.mlp.act_fn -> <class 'torch.nn.modules.activation.SiLU'>\n","model.layers.9.input_layernorm -> <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n","model.layers.9.post_attention_layernorm -> <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n","model.layers.10 -> <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n","model.layers.10.self_attn -> <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n","model.layers.10.self_attn.q_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.10.self_attn.k_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.10.self_attn.v_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.10.self_attn.o_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.10.mlp -> <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n","model.layers.10.mlp.gate_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.10.mlp.up_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.10.mlp.down_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.10.mlp.act_fn -> <class 'torch.nn.modules.activation.SiLU'>\n","model.layers.10.input_layernorm -> <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n","model.layers.10.post_attention_layernorm -> <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n","model.layers.11 -> <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n","model.layers.11.self_attn -> <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n","model.layers.11.self_attn.q_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.11.self_attn.k_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.11.self_attn.v_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.11.self_attn.o_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.11.mlp -> <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n","model.layers.11.mlp.gate_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.11.mlp.up_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.11.mlp.down_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.11.mlp.act_fn -> <class 'torch.nn.modules.activation.SiLU'>\n","model.layers.11.input_layernorm -> <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n","model.layers.11.post_attention_layernorm -> <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n","model.layers.12 -> <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n","model.layers.12.self_attn -> <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n","model.layers.12.self_attn.q_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.12.self_attn.k_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.12.self_attn.v_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.12.self_attn.o_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.12.mlp -> <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n","model.layers.12.mlp.gate_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.12.mlp.up_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.12.mlp.down_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.12.mlp.act_fn -> <class 'torch.nn.modules.activation.SiLU'>\n","model.layers.12.input_layernorm -> <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n","model.layers.12.post_attention_layernorm -> <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n","model.layers.13 -> <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n","model.layers.13.self_attn -> <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n","model.layers.13.self_attn.q_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.13.self_attn.k_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.13.self_attn.v_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.13.self_attn.o_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.13.mlp -> <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n","model.layers.13.mlp.gate_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.13.mlp.up_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.13.mlp.down_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.13.mlp.act_fn -> <class 'torch.nn.modules.activation.SiLU'>\n","model.layers.13.input_layernorm -> <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n","model.layers.13.post_attention_layernorm -> <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n","model.layers.14 -> <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n","model.layers.14.self_attn -> <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n","model.layers.14.self_attn.q_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.14.self_attn.k_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.14.self_attn.v_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.14.self_attn.o_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.14.mlp -> <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n","model.layers.14.mlp.gate_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.14.mlp.up_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.14.mlp.down_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.14.mlp.act_fn -> <class 'torch.nn.modules.activation.SiLU'>\n","model.layers.14.input_layernorm -> <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n","model.layers.14.post_attention_layernorm -> <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n","model.layers.15 -> <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n","model.layers.15.self_attn -> <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n","model.layers.15.self_attn.q_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.15.self_attn.k_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.15.self_attn.v_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.15.self_attn.o_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.15.mlp -> <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n","model.layers.15.mlp.gate_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.15.mlp.up_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.15.mlp.down_proj -> <class 'torch.nn.modules.linear.Linear'>\n","model.layers.15.mlp.act_fn -> <class 'torch.nn.modules.activation.SiLU'>\n","model.layers.15.input_layernorm -> <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n","model.layers.15.post_attention_layernorm -> <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n","model.norm -> <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n","model.rotary_emb -> <class 'transformers.models.llama.modeling_llama.LlamaRotaryEmbedding'>\n","lm_head -> <class 'torch.nn.modules.linear.Linear'>\n"]}]}]}
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0232120b",
   "metadata": {},
   "source": [
    "# RAG Integration with OpenWebUI\n",
    "# 🌐 OpenWebUI\n",
    "\n",
    "**OpenWebUI** is a fully open-source, self-hosted user interface designed to interact with Large Language Models (LLMs) like OpenAI, LLaMA, Mistral, Gemini, DeepSeek, and more — all through an intuitive, chat-style experience. It's built for developers, researchers, and organizations seeking a privacy-focused, extensible alternative to proprietary AI interfaces.\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Key Highlights\n",
    "\n",
    "- ✅ **Local-first LLM Interface** – No vendor lock-in.\n",
    "- 💬 **Multi-Model Chat Interface** – Use multiple LLMs with tabbed conversations.\n",
    "- 🧠 **Memory & Context Management** – Long-term memory per user, per thread.\n",
    "- 📚 **RAG Support** – Easily extend with custom Retrieval-Augmented Generation (RAG) backends.\n",
    "- 📦 **Plugin System** – Add custom filters, pipes, and integrations.\n",
    "- 🔒 **Private & Secure** – Full control over your data and infrastructure.\n",
    "- 🌐 **Multi-User Support** – Teams can use the same instance with role-based controls.\n",
    "- 🎨 **Responsive UI** – Optimized for desktop, mobile, and tablet.\n",
    "- 🛠️ **Dev-Friendly APIs** – Extend or embed easily in your own systems.\n",
    "\n",
    "---\n",
    "\n",
    "## 📘 What Can You Do with OpenWebUI?\n",
    "\n",
    "- Run **ChatGPT-style conversations** on your own infrastructure.\n",
    "- Integrate **custom RAG backends** for domain-specific document Q&A.\n",
    "- Switch between LLMs like GPT-4, Mistral, LLaMA, or DeepSeek on the fly.\n",
    "- Customize conversations with **system instructions, memory, and tools**.\n",
    "- Deploy internally in organizations for **private, secure AI access**.\n",
    "- Build **custom plugins** for translation, summarization, search, or any LLM task.\n",
    "\n",
    "---\n",
    "\n",
    "## 🏗️ Architecture Overview\n",
    "\n",
    "OpenWebUI is built using a modular architecture with the following layers:\n",
    "\n",
    "1. **Frontend (WebUI)**\n",
    "   - React-based, responsive design.\n",
    "   - Chat interface, settings, and plugin management.\n",
    "2. **Backend (API Layer)**\n",
    "   - Manages session, LLM calls, memory, and plugins.\n",
    "   - Written in Python with FastAPI.\n",
    "3. **Plugins**\n",
    "   - Filters (pre-processing)\n",
    "   - Pipes (mid-processing like RAG)\n",
    "   - Outlets (post-processing)\n",
    "4. **LLM Adapter**\n",
    "   - Interfaces with local/remote LLMs (Ollama, OpenAI, LM Studio, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "## 📦 Supported LLM Backends\n",
    "\n",
    "- 🌐 OpenAI (ChatGPT, GPT-4)\n",
    "- 🦙 Ollama (LLaMA, Mistral, DeepSeek, etc.)\n",
    "- 🔮 LM Studio\n",
    "- 🪄 Local GGUF models\n",
    "- 🧠 Any HTTP API with OpenAI-compatible schema\n",
    "\n",
    "---\n",
    "\n",
    "## 📁 Folder Structure\n",
    "\n",
    "---\n",
    "\n",
    "## 🔌 Plugin Ecosystem\n",
    "\n",
    "OpenWebUI supports a plugin-based architecture via:\n",
    "\n",
    "- **Filters** – Intercept input for moderation, formatting, rewriting.\n",
    "- **Pipes** – Inject RAG or dynamic content.\n",
    "- **Outlets** – Post-process output (e.g., rewrite, translate).\n",
    "- **Inlets** – Customize query routing logic.\n",
    "\n",
    "Plugins are written in Python and dropped into the `plugins/` directory. They are auto-loaded and configurable via the UI.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Example Use Cases\n",
    "\n",
    "- 🧑‍🏫 **Educational Assistant** – Connect to courseware and documents via RAG.\n",
    "- 🏢 **Enterprise Chatbot** – Hosted internally, supports fine-tuned LLMs.\n",
    "- 📖 **Document QA System** – Embed custom PDF Q&A workflows.\n",
    "- 🔧 **Dev Tooling** – Automate code generation, debugging, and explanations.\n",
    "- 🌍 **Multilingual Translator** – Use multiple LLMs for real-time translation.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Installation (Quick Start)\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/open-webui/open-webui\n",
    "cd open-webui\n",
    "docker-compose up --build \n",
    "```\n",
    "\n",
    "Alternatively, follow the below link to install the OpenWebUI\n",
    "https://docs.openwebui.com/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cbe059",
   "metadata": {},
   "source": [
    "# Follow the below steps to do the integration\n",
    "## Step-1 Open OpenWebUI and Then Go to Setting\n",
    "![\"Setting\"](images/Step-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e120bd4c",
   "metadata": {},
   "source": [
    "## Step-2: Further, Select Admin Settings from the setting\n",
    "![\"Admin Settings\"](images/Step-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84bed55",
   "metadata": {},
   "source": [
    "## Setp-3: Go to Functions and then click on the \"Create Function\" button.\n",
    "![\"Create FUnction\"](images/Step-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbf7315",
   "metadata": {},
   "source": [
    "## Step-4: Provide a function name and description for your functon.\n",
    "Further copy and past the below code in the coding window.\n",
    "![\"Functions\"](images/Step-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921cc593",
   "metadata": {},
   "source": [
    "## Use your RAG API in OpenWebUI\n",
    "Inside the model selection windows (which you chat window), select the model name to start your chat.\n",
    "![\"Utilize the RAG Workflow\"](images/Step-5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e3db16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "title: RAG Integration\n",
    "author: dr-avinash\n",
    "author_url: https://github.com/dr-avinash\n",
    "version: 1.0.0\n",
    "required_open_webui_version: 0.3.30\n",
    "\"\"\"\n",
    "\n",
    "from typing import List, Union, Generator, Iterator\n",
    "from pydantic import BaseModel, Field\n",
    "import requests\n",
    "\n",
    "\n",
    "class Pipe:\n",
    "    ###############################################################################################\n",
    "    class Valves(BaseModel):\n",
    "        rag_api_url: str = Field(\n",
    "            default=\"http://192.168.0.106:8000/rag\",  # Replace with your actual RAG endpoint\n",
    "            description=\"RAG API endpoint\",\n",
    "        )\n",
    "        enabled: bool = Field(default=True, description=\"Enable or disable RAG mode\")\n",
    "    ###############################################################################################\n",
    "    def __init__(self):\n",
    "        self.type = \"manifold\"\n",
    "        self.id = \"rag_integration\"\n",
    "        self.name = \"RAG\"\n",
    "        self.valves = self.Valves()\n",
    "    ###############################################################################################\n",
    "    def pipes(self) -> List[dict]:\n",
    "        if self.valves.enabled:\n",
    "            return [{\"id\": \"rag\", \"name\": \"-Demo\"}]\n",
    "        return []\n",
    "    ###############################################################################################\n",
    "    def pipe(self, body: dict, results=None) -> Union[str, Generator[str, None, None]]:\n",
    "        user_input = self._extract_user_input(body)\n",
    "        if not user_input:\n",
    "            yield \"No query provided to RAG system\"\n",
    "            return\n",
    "\n",
    "        print(f\"RAG Query: {user_input}\")\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                self.valves.rag_api_url, json={\"query\": user_input}, timeout=15\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            result = response.json()\n",
    "            final_text = result.get(\"response\", \"⚠️ No response from RAG API\")\n",
    "\n",
    "            # 🔁 Stream word by word\n",
    "            # for word in final_text.split():\n",
    "            #     yield word + \" \"\n",
    "            # 🔁 Stream character by character\n",
    "            for char in final_text:\n",
    "                yield char + \" \"\n",
    "        except Exception as e:\n",
    "            error_msg = f\"❌ Error contacting RAG API: {str(e)}\"\n",
    "            print(error_msg)\n",
    "            yield error_msg\n",
    "\n",
    "    ###############################################################################################\n",
    "    def _extract_user_input(self, body: dict) -> str:\n",
    "        messages = body.get(\"messages\", [])\n",
    "        if messages:\n",
    "            last_message = messages[-1]\n",
    "            if isinstance(last_message.get(\"content\"), list):\n",
    "                for item in last_message[\"content\"]:\n",
    "                    if item[\"type\"] == \"text\":\n",
    "                        return item[\"text\"]\n",
    "            else:\n",
    "                return last_message.get(\"content\", \"\")\n",
    "        return \"\"\n",
    "    ###############################################################################################\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

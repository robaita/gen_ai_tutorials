{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0403b71c",
   "metadata": {},
   "source": [
    "# Image Classification with Transfer Learning and Self-Attention\n",
    "\n",
    "In this notebook, we:\n",
    "- Explain what attention is, why it's useful\n",
    "- Understand the self-attention mechanism and its math\n",
    "- Implement transfer learning with a CNN (MobileNetV2) backbone\n",
    "- Add a custom self-attention layer to enhance representation learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4205b95a",
   "metadata": {},
   "source": [
    "## üß† What is Attention?\n",
    "Attention is a mechanism that allows neural networks to **focus on the most relevant parts** of the input when making predictions.\n",
    "\n",
    "In image classification, it helps the model weigh **which parts of an image are important** to make a decision ‚Äî rather than treating all pixels equally.\n",
    "\n",
    "Think of it like how humans focus their vision on specific objects in a scene, instead of looking at every part equally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9619e38a",
   "metadata": {},
   "source": [
    "## üî¨ Why Use Attention in Image Classification?\n",
    "Traditional CNNs use fixed receptive fields, but attention allows the model to:\n",
    "- Dynamically weigh features\n",
    "- Capture **long-range dependencies** (e.g., relation between different parts of an object)\n",
    "- Improve interpretability (you can visualize where the model focused)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693ea680",
   "metadata": {},
   "source": [
    "## üßÆ Self-Attention Math\n",
    "Given an input matrix $X \\in \\mathbb{R}^{T \\times D}$, where:\n",
    "- $T$ is the number of tokens (image patches or spatial locations)\n",
    "- $D$ is the embedding dimension\n",
    "\n",
    "We compute:\n",
    "1. $Q = XW^Q$ (queries)\n",
    "2. $K = XW^K$ (keys)\n",
    "3. $V = XW^V$ (values)\n",
    "\n",
    "Then attention weights:\n",
    "$$ A = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{D}}\\right) $$\n",
    "\n",
    "And the output:\n",
    "$$ Z = AV $$\n",
    "\n",
    "This gives us a new representation of each token as a **weighted combination of all tokens**, where the weights are based on similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f202efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Custom self-attention layer\n",
    "class SelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim  # D\n",
    "\n",
    "        # Dense layers to learn Wq, Wk, Wv (all shape: [D, D])\n",
    "        self.query_dense = layers.Dense(embed_dim)\n",
    "        self.key_dense   = layers.Dense(embed_dim)\n",
    "        self.value_dense = layers.Dense(embed_dim)\n",
    "        self.softmax = layers.Softmax(axis=-1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs shape: [B, T, D]\n",
    "\n",
    "        Q = self.query_dense(inputs)  # [B, T, D]\n",
    "        K = self.key_dense(inputs)    # [B, T, D]\n",
    "        V = self.value_dense(inputs)  # [B, T, D]\n",
    "\n",
    "        # Compute attention scores\n",
    "        attention_scores = tf.matmul(Q, K, transpose_b=True)  # [B, T, T]\n",
    "        d_k = tf.cast(tf.shape(K)[-1], tf.float32)\n",
    "        scaled_scores = attention_scores / tf.math.sqrt(d_k)  # scale by sqrt(D)\n",
    "\n",
    "        # Softmax to get weights\n",
    "        attention_weights = self.softmax(scaled_scores)       # [B, T, T]\n",
    "\n",
    "        # Weighted sum of values\n",
    "        output = tf.matmul(attention_weights, V)              # [B, T, D]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc43c99c",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Build the Model with MobileNetV2 + Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c9607f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_with_attention(num_classes=2):\n",
    "    base_model = tf.keras.applications.MobileNetV2(\n",
    "        input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
    "    base_model.trainable = False\n",
    "\n",
    "    inputs = layers.Input(shape=(224, 224, 3))\n",
    "    x = tf.keras.applications.mobilenet_v2.preprocess_input(inputs)\n",
    "    x = base_model(x)                             # [B, 7, 7, 1280]\n",
    "    x = layers.Reshape((49, 1280))(x)             # [B, T=49, D=1280]\n",
    "\n",
    "    x = SelfAttention(embed_dim=1280)(x)          # [B, 49, 1280]\n",
    "    x = layers.GlobalAveragePooling1D()(x)        # [B, 1280]\n",
    "    x = layers.Dense(256, activation='relu')(x)   # [B, 256]\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)  # [B, num_classes]\n",
    "\n",
    "    return models.Model(inputs, outputs)\n",
    "\n",
    "model = build_model_with_attention(num_classes=2)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28517c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457fb8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 files belonging to 2 classes.\n",
      "Found 1000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "import os\n",
    "\n",
    "# Download the dataset from Microsoft (hosted by TensorFlow)\n",
    "url = \"https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\"\n",
    "path_to_zip = tf.keras.utils.get_file('cats_and_dogs_filtered.zip', origin=url, extract=True)\n",
    "\n",
    "# Get dataset path\n",
    "dataset_dir = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')\n",
    "\n",
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "val_dir = os.path.join(dataset_dir, 'validation')\n",
    "\n",
    "# Load data into TensorFlow datasets\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = image_dataset_from_directory(train_dir,\n",
    "                                             shuffle=True,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             image_size=(IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "val_dataset = image_dataset_from_directory(val_dir,\n",
    "                                           shuffle=True,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           image_size=(IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "# Prefetch for performance\n",
    "train_dataset = train_dataset.prefetch(buffer_size=32)\n",
    "val_dataset = val_dataset.prefetch(buffer_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d385dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(train_dataset, epochs=5, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68305f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy and loss\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(len(acc))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Train Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Val Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Train Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Val Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_2.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

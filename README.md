# ðŸš€ Generative AI Training Program Overview

This repository presents a comprehensive and modular **Generative AI Training Plan**, designed to equip learners with both foundational knowledge and advanced skills in modern AI systems. The multi-week curriculum systematically covers theory, architecture, and practical applications of LLMs, multimodal AI, and autonomous agents.

## ðŸ§  Key Components

### ðŸ”¹ AI Ecosystem & Evolution  
Understand the progression from traditional AI to Deep Learning, Reinforcement Learning, and Generative AI, with a detailed look at the LLM ecosystem and evaluation methods.

### ðŸ”¹ Prompt Engineering  
Learn and apply techniques like **zero-shot**, **few-shot**, **chain-of-thought**, and **tree-of-thought** prompting to harness LLM capabilities effectively.

### ðŸ”¹ Deep Learning Foundations  
Explore core concepts including **multilayer perceptrons (MLPs)**, **activation functions**, **backpropagation**, and **RNNs**.

### ðŸ”¹ Transformers & Attention  
In-depth coverage of **self-attention**, **transformer architecture**, and **encoder-decoder variants** powering todayâ€™s state-of-the-art models.

### ðŸ”¹ LLM Internals & Fine-Tuning  
Dive into **LLM architecture**, **training strategies**, and techniques for customizing models to domain-specific tasks.

### ðŸ”¹ Agentic AI  
Introduction to **autonomous AI agents** that perform goal-driven tasks with memory, reasoning, and tool usage capabilities.

### ðŸ”¹ RAG (Retrieval-Augmented Generation)  
Learn to integrate LLMs with external knowledge sources for **dynamic, context-rich response generation** in enterprise applications.

### ðŸ”¹ MCP (Multimodal Co-Pilot)  
Explore the future of AI through **multimodal systems** that combine **text, vision, and voice** for next-generation user experiences.

---

Whether you're a researcher, engineer, or enthusiast, this training plan provides a guided pathway to mastering the theory and practice of **Generative AI** and its real-world applications.

# Course Content

| Lecture Name | Topic Covered |
|--------------|---------------|
| Lecture-1 | History of AI, Paradigm Shifts in AI, AI vs ML vs DL, Generative AI, Reinforcement Learning and Autonomous Agents |
| Lecture-2 | Zero shot, Few Shot, Chain of Thought, Tree of Thought, LLM as Judge |
| Lecture-3 | Neuron architecture (input, weights, bias, output), Forward propagation in MLP, Multi-layer structure (hidden layers), Weight initialization |
| Lecture-4 | Chain rule for gradients, Error function (loss), Gradient flow across layers, Optimization: SGD, Adam, etc |
| Lecture-5 | RNN architecture and sequential data, Time-step unrolling, Gradient flow in RNNs, Layer Normalization vs Batch Normalization |
| Lecture-6 | Attention Mechanism, Self-Attention, Cross Attention |
| Lecture-7 | Data Collection and Preprocessing, Tokenization and Vocabulary Creation, Model Architecture and Configuration, Pretraining Objectives, Fine-Tuning and Alignment Techniques |
| Lecture-8 | Text-Image Fusion, Vision-Language Models, Multimodal Prompting |
| Lecture-9 | Latent Diffusion, Denoising Score Matching, Forward and Reverse Process |
| Lecture-10 | Understanding Component Pipelines, Orchestrating LLMs with Tools/Agents, Security, Guardrails & Privacy |
| Lecture-11 | Limitation of Traditional RAG |
| Lecture-12 | Entity linking and node mapping, Graph construction from documents, Path-based context retrieval, Integration with knowledge graphs |
| Lecture-13 | Dynamic header generation, Embedding-aware section titles, Enhancing retrieval relevance |
| Lecture-14 | Summarization-based token reduction, Importance-aware trimming, LLM-assisted compression |
| Lecture-15 | Data formatting and tokenization, Instruction tuning basics, When and why to finetune |
| Lecture-16 | Weight precision reduction (e.g., INT8), Trade-off: speed vs accuracy, Hardware-aware optimizations |
| Lecture-17 | Comparison between Langgraph, Crew AI, LlamaIndex, Smal Agent, Auto Gen |
| Lecture-18 | Build your A2A |
